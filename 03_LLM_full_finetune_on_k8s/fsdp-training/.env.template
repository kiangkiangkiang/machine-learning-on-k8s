#!/bin/bash

# Configuration for FSDP jobs

## AWS
export AWS_REGION=<your-region>
export ACCOUNT=$(aws sts get-caller-identity --query Account --output text)

## Docker Image
export REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com
export ECR_REPO_FOLDER=<your-ecr-folder>
export DOCKER_IMAGE_NAME=<your-image>

# K8s Deployment
export TRAINING_APP_NAME_1="fsdp-node-1" 
export TRAINING_APP_NAME_2="fsdp-node-2" 
export TRAINING_NAMESPACE="fsdp-llama-training-ns"
export NUM_WORKERS=2 ## NUM_WORKERS - Default 2, set to number of worker nodes
export GPU_PER_WORKER=1 ## GPU_PER_WORKER - number of GPUs per worker, the number of GPUs for the selected instance type.

# RDZV: c10d
export RDZV_PORT=12355
export RDZV_ENDPOINT="${TRAINING_APP_NAME_1}.${TRAINING_NAMESPACE}.svc.cluster.local:${RDZV_PORT}"

# Model
export HF_TOKEN=<your-huggingface-token>
export MODEL_NAME=meta-llama/Llama-2-7b-hf

export CMD="huggingface-cli login --token ${HF_TOKEN} && torchrun --nproc_per_node=${GPU_PER_WORKER} --nnodes=${NUM_WORKERS} --rdzv-endpoint=${RDZV_ENDPOINT} --rdzv-backend=c10d --local_addr \${LOCAL_ADDR} finetune.py --use_fp16 --num_epochs=3 --batch_size_training=3 --enable_fsdp --model_name $MODEL_NAME --output_dir . --samsum_dataset.trust_remote_code True --max_train_step 2"
export IMAGE_PULL_SECET=<your secret name>
